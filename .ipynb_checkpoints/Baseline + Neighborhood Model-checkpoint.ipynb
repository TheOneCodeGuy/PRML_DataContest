{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR BASELINE PREDICION, STORING AND SUBMISSION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_gradient_descent(n_epochs, batch_size, data, eta):\n",
    "    \n",
    "    data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # Unknown parameters\n",
    "    a = np.zeros(n_users)  # User variations\n",
    "    b = np.zeros(n_movies) # Movie variations\n",
    "    \n",
    "    mu = np.mean(data['rating'])\n",
    "    predictions = np.zeros(len(data))\n",
    "    n_batches = int(len(data)/batch_size)\n",
    "    \n",
    "    # Create indices for every batch\n",
    "    # Of the form [-1, batch_size, 2*batchsize, ... len(data)]\n",
    "    indices = [batch_size*x for x in range(n_batches)]\n",
    "    indices[0] = -1\n",
    "    indices.append(len(data) - 1)\n",
    "            \n",
    "    # Gradient Descent Begins\n",
    "    print('Batch Size - ', batch_size, ', Eta - ', eta)\n",
    "    exit = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # Display for every epoch - Epoch number and error\n",
    "        print('Epoch-',(epoch+1),'/',n_epochs)\n",
    "        err = np.mean(abs(data['rating'] - predictions));\n",
    "        print('Error =',err)\n",
    "\n",
    "        # For every mini-batch\n",
    "        for batch_no in range(n_batches):\n",
    "\n",
    "            # Select mini-batch\n",
    "            batch = data.loc[indices[batch_no]+1:indices[batch_no+1]]\n",
    "\n",
    "            # Display after every 10% completetion\n",
    "            if (batch_no+1) % (int(n_batches/10)) == 0:\n",
    "                print('Completetion - ', np.ceil((batch_no+1)/n_batches*100), '\\b%')\n",
    "                curr_err = (np.mean(abs(predictions[predictions!=0] - data.loc[(predictions!=0), 'rating'])))\n",
    "                print(curr_err)\n",
    "                if (curr_err == np.inf) or np.isnan(curr_err):\n",
    "                    exit = 1\n",
    "                    print('Breaking Out...')\n",
    "                    break\n",
    "       \n",
    "            # Predictions for the selected mini-batch using the current a and b\n",
    "            for index, x in batch.iterrows():\n",
    "                predictions[index] = mu + a[int(x['userId'])] + b[int(x['movieId'])]\n",
    "\n",
    "            # Updating a and b using the predictions on the current mini-batch\n",
    "            for index, x in batch.iterrows():\n",
    "                mu = mu + eta * (x['rating'] - predictions[index])\n",
    "                a[int(x['userId'])] = a[int(x['userId'])] + eta * (x['rating'] - predictions[index])\n",
    "                b[int(x['movieId'])] = b[int(x['movieId'])] + eta * (x['rating'] - predictions[index])\n",
    "            \n",
    "        if exit == 1:\n",
    "            exit = 0\n",
    "            print('Very large error - Moving to next search')\n",
    "            break\n",
    "\n",
    "    return (a, b, mu)\n",
    "\n",
    "# To predict using baseline model\n",
    "def baseline_predict(data, a, b, mu):\n",
    "    \n",
    "    data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for index, point in data.iterrows():\n",
    "        predictions[index] = mu + a[int(point['userId'])] + b[int(point['movieId'])]\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# To store baseline paramters for future use\n",
    "def store_baseline_params(a, b, mu, param_filename):\n",
    "    mu_vec = [np.nan]*(len(a))\n",
    "    mu_vec[0] = mu\n",
    "    params = {'a': a,\n",
    "              'b': b,\n",
    "              'mu': mu_vec}\n",
    "\n",
    "    params = data.from_dict(params, orient = 'columns')\n",
    "    params.to_csv(param_filename, index = False)\n",
    "    \n",
    "# Convert predictions to CSV\n",
    "def submit(prediction, file_name):\n",
    "    submission = test.from_dict(prediction, orient='index', columns=['Prediction'])\n",
    "    submission[submission['Prediction'] < 0.5] = 0.5\n",
    "    submission[submission['Prediction'] > 5] = 5\n",
    "    submission.to_csv(file_name, index_label = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE HYPERPARAMTER SEARCH\n",
    "\n",
    "# Import train files\n",
    "filename = 'data/train.csv'\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Splitting into train and CV\n",
    "indices = np.random.permutation(len(data))\n",
    "train_length = int(len(data)*0.8)\n",
    "train_data = data.loc[indices[0:train_length]]\n",
    "test_data = data.loc[indices[(train_length+1):]]\n",
    "\n",
    "n_movies = 10000\n",
    "n_users = 10000\n",
    "\n",
    "# Gradiet Descent Hyperparameters\n",
    "n_epochs = 3\n",
    "\n",
    "min_err = np.inf\n",
    "\n",
    "for batch_size in [500, 750, 1000, 2000]:\n",
    "    for eta in [0.0001, 0.0005, 0.001]:\n",
    "        (a, b, mu) = run_gradient_descent(n_epochs, batch_size, train_data, eta)\n",
    "        test_pred = predict(test_data, a, b, mu)\n",
    "        error = np.mean(abs(test_data['rating'] - test_pred.values))\n",
    "        if error < min_err:\n",
    "            min_err = error\n",
    "            selection = (batch_size, eta)\n",
    "\n",
    "print(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Choice\n",
    "**Batch Size** = 750  \n",
    "**Learning Rate** = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE TRAINING, PREDICTION AND SUBMISSION\n",
    "\n",
    "selection = (750, 0.001)\n",
    "\n",
    "# Gradient Descent with 10 epochs\n",
    "(a, b, mu) = run_gradient_descent(10, selection[0], train_data, selection[1])\n",
    "\n",
    "# Store a, b, mu for future use\n",
    "store_baseline_params(a, b, mu, 'Storage/Baseline_Params.csv')\n",
    "\n",
    "# Import test files\n",
    "filename = 'data/test.csv'\n",
    "test = pd.read_csv(filename)\n",
    "\n",
    "# Get predictions\n",
    "baseline_prediction = predict(test, a, b, mu)\n",
    "\n",
    "# SUBMISSION\n",
    "submit(baseline_prediction, 'Submissions/Baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "9%\n",
      "18%\n",
      "28%\n",
      "37%\n",
      "47%\n",
      "56%\n",
      "66%\n",
      "75%\n",
      "85%\n",
      "94%\n"
     ]
    }
   ],
   "source": [
    "# NEIGHBORHOOD MODEL RATING MATRIX AND SIMILARITY MATRIX\n",
    "\n",
    "# Hyperparameters for Neighborhood model\n",
    "lambda2 = 100\n",
    "k = 100\n",
    "\n",
    "# Rating Matrix for faster operation\n",
    "A = np.zeros([n_users, n_movies])\n",
    "for ind, x in data.iterrows():\n",
    "    A[int(x['userId']), int(x['movieId'])] = x['rating']\n",
    "    \n",
    "    # Display Status\n",
    "    if(ind%500000 == 0):\n",
    "        print(int(ind*100/len(data)), '\\b%')\n",
    "\n",
    "# Similarity Matrix of Shrunk Coefficients\n",
    "S = np.zeros([n_movies, n_movies])\n",
    "i = 1\n",
    "for m1 in range(n_movies):\n",
    "    for m2 in range(m1):\n",
    "        # Find indices of common users between m1 and m2\n",
    "        ind = (np.multiply(A[:, m1], A[:, m2]) != 0)\n",
    "        \n",
    "        # m1 ratings by common users\n",
    "        x = A[ind, m1]\n",
    "        n = len(x)\n",
    "        \n",
    "        # If less than 2 common elements, Pearson's correlation coefficient not defined\n",
    "        if n > 1:\n",
    "            # m2 ratings by common users\n",
    "            y = A[ind, m2]\n",
    "            \n",
    "            # Correlation Coefficient\n",
    "            p = np.corrcoef(x, y)[0,1]\n",
    "            \n",
    "            # Shrunk Coefficient - Symmetric Matrix\n",
    "            S[m2, m1] = n/(n + lambda2)*p\n",
    "            S[m1, m2] = n/(n + lambda2)*p\n",
    "            \n",
    "        # Display Status\n",
    "        if (i % 10000) == 0:\n",
    "            print(200*i/(n_movies*n_movies), '\\b%')\n",
    "        i = i + 1\n",
    "\n",
    "# Store S for future use\n",
    "def store_S(S, S_filename):\n",
    "    cols = [str(e) for e in range(10000)]\n",
    "    S_df = pd.DataFrame(S, columns=cols, index=cols)\n",
    "    S_df.to_csv('shrunk_coeff.csv')\n",
    "    \n",
    "store_S(S, 'Storage/shrunk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will be deleted before submission\n",
    "\n",
    "# baseline_params = pd.read_excel('BaselineParameters.xlsx', index_col=None, header=None)\n",
    "# a = baseline_params[0]\n",
    "# b = baseline_params[1]\n",
    "# mu = baseline_params.loc[0, 2]\n",
    "\n",
    "# Convert predictions to CSV\n",
    "def submit(prediction, file_name):\n",
    "    submission = test.from_dict(prediction, orient='index', columns=['Prediction'])\n",
    "    submission[submission['Prediction'] < 0.5] = 0.5\n",
    "    submission[submission['Prediction'] > 5] = 5\n",
    "    submission.to_csv(file_name, index_label = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "0.43384173800470976%\n",
      "0.8676834760094195%\n",
      "1.3015252140141294%\n",
      "1.735366952018839%\n",
      "2.1692086900235488%\n",
      "2.6030504280282587%\n",
      "3.036892166032968%\n",
      "3.470733904037678%\n",
      "3.9045756420423885%\n",
      "4.3384173800470975%\n",
      "4.7722591180518075%\n",
      "5.206100856056517%\n",
      "5.639942594061227%\n",
      "6.073784332065936%\n",
      "6.507626070070646%\n",
      "6.941467808075356%\n",
      "7.375309546080066%\n",
      "7.809151284084777%\n",
      "8.242993022089486%\n",
      "8.676834760094195%\n",
      "9.110676498098906%\n",
      "9.544518236103615%\n",
      "9.978359974108326%\n",
      "10.412201712113035%\n",
      "10.846043450117744%\n",
      "11.279885188122455%\n",
      "11.713726926127164%\n",
      "12.147568664131873%\n",
      "12.581410402136584%\n",
      "13.015252140141293%\n",
      "13.449093878146003%\n",
      "13.882935616150712%\n",
      "14.316777354155422%\n",
      "14.750619092160132%\n",
      "15.184460830164841%\n",
      "15.618302568169554%\n",
      "16.052144306174263%\n",
      "16.485986044178972%\n",
      "16.91982778218368%\n",
      "17.35366952018839%\n",
      "17.7875112581931%\n",
      "18.221352996197812%\n",
      "18.65519473420252%\n",
      "19.08903647220723%\n",
      "19.522878210211942%\n",
      "19.95671994821665%\n",
      "20.39056168622136%\n",
      "20.82440342422607%\n",
      "21.25824516223078%\n",
      "21.692086900235488%\n",
      "22.1259286382402%\n",
      "22.55977037624491%\n",
      "22.99361211424962%\n",
      "23.427453852254327%\n",
      "23.861295590259036%\n",
      "24.295137328263746%\n",
      "24.728979066268458%\n",
      "25.162820804273167%\n",
      "25.596662542277876%\n",
      "26.030504280282585%\n",
      "26.464346018287298%\n",
      "26.898187756292007%\n",
      "27.332029494296716%\n",
      "27.765871232301425%\n",
      "28.199712970306134%\n",
      "28.633554708310843%\n",
      "29.06739644631556%\n",
      "29.501238184320265%\n",
      "29.935079922324974%\n",
      "30.368921660329683%\n",
      "30.802763398334392%\n",
      "31.236605136339108%\n",
      "31.670446874343817%\n",
      "32.104288612348526%\n",
      "32.538130350353235%\n",
      "32.971972088357944%\n",
      "33.40581382636265%\n",
      "33.83965556436736%\n",
      "34.27349730237207%\n",
      "34.70733904037678%\n",
      "35.14118077838149%\n",
      "35.5750225163862%\n",
      "36.00886425439091%\n",
      "36.442705992395624%\n",
      "36.87654773040033%\n",
      "37.31038946840504%\n",
      "37.74423120640975%\n",
      "38.17807294441446%\n",
      "38.61191468241917%\n",
      "39.045756420423885%\n",
      "39.479598158428594%\n",
      "39.9134398964333%\n",
      "40.34728163443801%\n",
      "40.78112337244272%\n",
      "41.21496511044743%\n",
      "41.64880684845214%\n",
      "42.08264858645685%\n",
      "42.51649032446156%\n",
      "42.950332062466266%\n",
      "43.384173800470975%\n",
      "43.81801553847569%\n",
      "44.2518572764804%\n",
      "44.68569901448511%\n",
      "45.11954075248982%\n",
      "45.55338249049453%\n",
      "45.98722422849924%\n",
      "46.421065966503946%\n",
      "46.854907704508655%\n",
      "47.288749442513364%\n",
      "47.72259118051807%\n",
      "48.15643291852278%\n",
      "48.59027465652749%\n",
      "49.02411639453221%\n",
      "49.457958132536916%\n",
      "49.891799870541625%\n",
      "50.325641608546334%\n",
      "50.75948334655105%\n",
      "51.19332508455575%\n",
      "51.62716682256047%\n",
      "52.06100856056517%\n",
      "52.49485029856989%\n",
      "52.928692036574596%\n",
      "53.362533774579305%\n",
      "53.796375512584014%\n",
      "54.23021725058872%\n",
      "54.66405898859343%\n",
      "55.09790072659815%\n",
      "55.53174246460285%\n",
      "55.965584202607566%\n",
      "56.39942594061227%\n",
      "56.833267678616984%\n",
      "57.267109416621686%\n",
      "57.7009511546264%\n",
      "58.13479289263112%\n",
      "58.56863463063582%\n",
      "59.00247636864053%\n",
      "59.43631810664524%\n",
      "59.87015984464995%\n",
      "60.304001582654664%\n",
      "60.737843320659366%\n",
      "61.17168505866408%\n",
      "61.605526796668784%\n",
      "62.0393685346735%\n",
      "62.473210272678216%\n",
      "62.90705201068292%\n",
      "63.340893748687634%\n",
      "63.774735486692336%\n",
      "64.20857722469705%\n",
      "64.64241896270175%\n",
      "65.07626070070647%\n",
      "65.51010243871119%\n",
      "65.94394417671589%\n",
      "66.3777859147206%\n",
      "66.8116276527253%\n",
      "67.24546939073002%\n",
      "67.67931112873472%\n",
      "68.11315286673944%\n",
      "68.54699460474414%\n",
      "68.98083634274886%\n",
      "69.41467808075356%\n",
      "69.84851981875828%\n",
      "70.28236155676298%\n",
      "70.7162032947677%\n",
      "71.1500450327724%\n",
      "71.58388677077711%\n",
      "72.01772850878181%\n",
      "72.45157024678653%\n",
      "72.88541198479125%\n",
      "73.31925372279595%\n",
      "73.75309546080067%\n",
      "74.18693719880537%\n",
      "74.62077893681008%\n",
      "75.0546206748148%\n",
      "75.4884624128195%\n",
      "75.92230415082422%\n",
      "76.35614588882892%\n",
      "76.78998762683364%\n",
      "77.22382936483834%\n",
      "77.65767110284305%\n",
      "78.09151284084777%\n",
      "78.52535457885247%\n",
      "78.95919631685719%\n",
      "79.39303805486189%\n",
      "79.8268797928666%\n",
      "80.26072153087131%\n",
      "80.69456326887602%\n",
      "81.12840500688073%\n",
      "81.56224674488544%\n",
      "81.99608848289014%\n",
      "82.42993022089486%\n",
      "82.86377195889956%\n",
      "83.29761369690428%\n",
      "83.73145543490898%\n",
      "84.1652971729137%\n",
      "84.5991389109184%\n",
      "85.03298064892311%\n",
      "85.46682238692783%\n",
      "85.90066412493253%\n",
      "86.33450586293725%\n",
      "86.76834760094195%\n",
      "87.20218933894667%\n",
      "87.63603107695138%\n",
      "88.06987281495608%\n",
      "88.5037145529608%\n",
      "88.9375562909655%\n",
      "89.37139802897022%\n",
      "89.80523976697492%\n",
      "90.23908150497964%\n",
      "90.67292324298435%\n",
      "91.10676498098906%\n",
      "91.54060671899377%\n",
      "91.97444845699847%\n",
      "92.40829019500319%\n",
      "92.84213193300789%\n",
      "93.27597367101261%\n",
      "93.70981540901731%\n",
      "94.14365714702203%\n",
      "94.57749888502673%\n",
      "95.01134062303144%\n",
      "95.44518236103615%\n",
      "95.87902409904086%\n",
      "96.31286583704556%\n",
      "96.74670757505028%\n",
      "97.18054931305498%\n",
      "97.6143910510597%\n",
      "98.04823278906441%\n",
      "98.48207452706912%\n",
      "98.91591626507383%\n",
      "99.34975800307853%\n",
      "99.78359974108325%\n"
     ]
    }
   ],
   "source": [
    "def neighborhood_predict(data, test, A, S, k, a, b, mu):\n",
    "    \n",
    "    prediction = {}\n",
    "\n",
    "    movieIds = np.unique(data['movieId'])\n",
    "    userIds = np.unique(data['userId'])\n",
    "\n",
    "    # Matrix with a_ij = 1 if i has rated j, 0 otherwise\n",
    "    binary_A = A.copy()\n",
    "    binary_A[binary_A != 0] = 1\n",
    "\n",
    "    # Predict for test\n",
    "    for index, point in test.iterrows():\n",
    "\n",
    "        movie = point['movieId']\n",
    "        user = point['userId']\n",
    "\n",
    "        # Number of movies rated by user\n",
    "        n_rated = np.count_nonzero(binary_A[user, :])\n",
    "        \n",
    "        # New user\n",
    "        if n_rated == 0:\n",
    "            prediction[index] = mu + a[user] + b[movie]\n",
    "\n",
    "        # Old user and old movie\n",
    "        if n_rated > 0:\n",
    "            \n",
    "            # Similarity between movie and other movies rated by user\n",
    "            S_movies = np.multiply(binary_A[user, :], S[movie, :])\n",
    "            \n",
    "            # Check if atleast k movies have been rated by user - if not select all movies rated\n",
    "            kk = min(k, n_rated)\n",
    "            k_ind = np.argpartition(-abs(S_movies), (kk - 1))[:kk]\n",
    "\n",
    "            # Prediction\n",
    "            S_Sum = 0\n",
    "            pred = 0\n",
    "\n",
    "            for i in k_ind:\n",
    "                baseline = mu + a[user] + b[i]\n",
    "                pred = pred + S[movie, i] * (A[user, i] - baseline)\n",
    "                S_Sum = abs(S_Sum) + S[movie, i]\n",
    "            \n",
    "            if S_Sum != 0:\n",
    "                prediction[index] = mu + a[user] + b[movie] + pred/S_Sum\n",
    "            \n",
    "            # New Movie\n",
    "            else:\n",
    "                prediction[index] = mu + a[user] + b[movie]\n",
    "        \n",
    "        # Display Status\n",
    "        if (index % 10000) == 0:\n",
    "            print(index/len(test)*100,'\\b%')\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Predict\n",
    "neighborhood_pred = neighborhood_predict(data, test, A, S, k, a, b, mu)\n",
    "\n",
    "# SUBMISSION\n",
    "submit(neighborhood_pred, 'Submissions/Neighborhood2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
